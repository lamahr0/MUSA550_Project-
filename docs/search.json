[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Background Information",
    "section": "",
    "text": "Urban Parks Info\n\n:::"
  },
  {
    "objectID": "analysis/1-Demographics_Study.html",
    "href": "analysis/1-Demographics_Study.html",
    "title": "Urban Parks & Demographics",
    "section": "",
    "text": "Code\n#Data Collection\n#Methods to analyze\n#results \n\n\n\n\nCode\n#imports\nimport seaborn as sns \nimport pandas as pd\nimport numpy as np \nfrom matplotlib import pyplot as plt\nimport altair as alt\nimport geopandas as gpd\nimport json\nimport folium\nimport pygris\nimport cenpy\nimport re\nimport copy\nimport warnings\nimport holoviews as hv\nimport hvplot.pandas \nfrom holoviews import opts\nhv.extension('bokeh')\nimport ipywidgets as widgets\nfrom ipywidgets import interact\nfrom scipy.stats import pearsonr\n# Suppress all warnings\nwarnings.filterwarnings('ignore')\nfrom folium import Map, Marker\nfrom IPython.display import IFrame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Check Parks\n#read shape file \nshape_file_loc =  'parks_map/geo_export_46a7de00-0067-42f5-a6bc-bec64e5a0f0b.shp'\n\n#convert it into geopanda dataframe \ndef get_gpd_df(use_shape_file=True):\n    if use_shape_file:\n        gdf = gpd.read_file(shape_file_loc)\n    return gdf\nparks_gpd =  get_gpd_df()\n\n#plot the map \npark_map = parks_gpd.explore(\n    tiles=\"Cartodb positron\",\n        style_kwds={\n        \"weight\": 2,\n        \"color\": \"green\",\n        \"fillOpacity\": 0.5,\n        }\n)\nfolium.TileLayer(opacity=0.20).add_to(park_map)\n#create api conncection\nny_state_code = \"36\"\nvariables = ['NAME', 'B16008_002E' #Native population\n             , 'B16008_019E' #Foreign-born population\n             , 'B25064_001E' , #median_rent\n             'B19013_001E'] #median_income\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nNY_demo_data2021 = acs.query(\n    cols=variables,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": ny_state_code,  \"county\":  \"*\"},\n)\nacs2 = cenpy.remote.APIConnection(\"ACSDT5Y2010\")\nNY_demo_data2010 = acs2.query(\n    cols=variables,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": ny_state_code, \"county\": \"*\"},\n)\n\nNY_demo_data2010.dropna(inplace=True)\nNY_demo_data2021.dropna(inplace=True)\n\n#\"061\" Manhattan (New York County) 005 #Bronx 081 Queen  047  Brooklyn (Kings County) 085 Staten Island (Richmond County)\nny_city_counties = [\"061\" , \"005\" , \"081\", \"047\", \"085\"]\n\nNY_demo_data2010 = NY_demo_data2010[NY_demo_data2010['county'].isin(ny_city_counties)]\n\n\nfor variable in variables:\n    if variable != \"NAME\":\n        NY_demo_data2010[variable] = NY_demo_data2010[variable].astype(float)\n\nfor variable in variables:\n    if variable != \"NAME\":\n        NY_demo_data2021[variable] = NY_demo_data2021[variable].astype(float)\n        \nNY_demo_data2010['population2010'] = NY_demo_data2010['B16008_002E'] + NY_demo_data2010['B16008_019E']\nNY_demo_data2010 = NY_demo_data2010.rename(columns={'B19013_001E': 'median_income2010' ,'B25064_001E': 'median_rent2010'})\nNY_demo_data2010 = NY_demo_data2010[NY_demo_data2010['population2010']&gt;0]\nNY_demo_data2010 = NY_demo_data2010[NY_demo_data2010['median_income2010']&gt;=0]\n\nNY_demo_data2021['population2021'] = NY_demo_data2021['B16008_002E'] + NY_demo_data2021['B16008_019E']\nNY_demo_data2021 = NY_demo_data2021.rename(columns={'B19013_001E': 'median_income2021' ,'B25064_001E': 'median_rent2021'})\nNY_demo_data2021 = NY_demo_data2021[NY_demo_data2021['population2021']&gt;0]\nNY_demo_data2021 = NY_demo_data2021[NY_demo_data2021['median_income2021']&gt;=0]\n\n\npopulation_total_df = pd.merge(NY_demo_data2010, NY_demo_data2021 , on=['tract','county','NAME']) \n\npopulation_total_df['change_precent'] = ((population_total_df['population2021'] - population_total_df['population2010']) /(np.absolute(population_total_df['population2010'])))*100\n\nshape_file_loc =  'census_tract_shapefile/tl_2021_36_tract.shp'\n#convert it into geopanda dataframe \ndef get_gpd_df(use_shape_file=True):\n    if use_shape_file:\n        gdf = gpd.read_file(shape_file_loc)\n    return gdf\nnyc_gpd =  get_gpd_df()\n\nnyc_demo_merged = nyc_gpd.merge(\n    population_total_df,\n    left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\"],\n    right_on=[\"state_x\", \"county\", \"tract\"],)\n\nnyc_pct_change_mean = nyc_demo_merged['change_precent'].mean()\n\nnyc_demo_merged = nyc_demo_merged[['tract', 'geometry', 'population2010','population2021','change_precent'\n                                  ,'median_income2010' , 'median_income2021',\n                                  'median_rent2010','median_rent2021']]\nnyc_demo_merged = nyc_demo_merged[nyc_demo_merged['change_precent'] &lt; 300]\n\n\n\n\nCode\ndef style(feature):\n    return {\n        'fillColor': 'green',  \n        'color': 'black',      \n        'weight': 2,          \n        'fillOpacity': 0.6    \n    }\ndef mapping(df,col,parks_gpd=parks_gpd):\n\n    m = df.explore(column=col,cmap = 'Blues', \n                                tiles=\"CartoDB positron\", zoom_start=11)\n    folium.Marker(location=[40.747993, -74.004890], popup=\"The High Line\" , icon=folium.Icon(icon='tree' ,color='red')).add_to(m)\n    folium.Marker(location=[40.785091, -73.968285], popup=\"Central Park\" , icon=folium.Icon(icon='tree' ,color='red')).add_to(m)\n    folium.Marker(location=[40.665535, -73.969749], popup=\"Prospect Park\" , icon=folium.Icon(icon='tree' ,color='red')).add_to(m)\n    folium.Marker(location=[40.699215, -73.999039], popup=\"Brooklyn Bridge Park\" , icon=folium.Icon(icon='tree' ,color='red')).add_to(m)\n    folium.Marker(location=[40.739716, -73.840782], popup=\"Flushing Meadows Corona Park\" , icon=folium.Icon(icon='tree' ,color='red')).add_to(m)\n    folium.Marker(location=[40.703564, -74.016678], popup=\"Battery Park\" , icon=folium.Icon(icon='tree' ,color='red')).add_to(m)\n\n    folium.GeoJson(\n        parks_gpd,\n        name='geojson_layer',\n        style_function=style\n    ).add_to(m)\n    \n    return m \n\n\n\nDo urban parks influence migration to nearby neighborhoods?\n\n\nCode\nm = mapping(nyc_demo_merged,\"change_precent\",parks_gpd=parks_gpd)\n# Save the map as HTML\nhtml_path = \"map1.html\"\nm.save(html_path)\n# Display the HTML file as an iframe\nIFrame(html_path, width=800, height=600)\n\n\n\n        \n        \n\n\n\n\nDid neighborhoods nearby urban parks experience a decrease in population between Census 2010 and Census 2020?\n\n\nCode\nnyc_demo_merged_sub_loss = nyc_demo_merged[nyc_demo_merged['change_precent'] &lt; 0]\nm2 = mapping(nyc_demo_merged_sub_loss,\"change_precent\",parks_gpd=parks_gpd)\n# Save the map as HTML\nhtml_path = \"map2.html\"\nm2.save(html_path)\n# Display the HTML file as an iframe\nIFrame(html_path, width=800, height=600)\n#decreased \n\n\n\n        \n        \n\n\n\n\nDoes the variation in rental prices contribute to the population decline in areas experiencing a decrease?\n\n\nCode\ndef compare(df, tracts , park_name):\n    df.tract = df.tract.astype(str)\n    df = df[df.tract.isin(tracts)]\n    df = df[['median_income2010', 'median_income2021','median_rent2010' , 'median_rent2021']]\n    mean = df.mean(axis=0).reset_index()\n    mean = mean.rename(columns={\n    'index': 'Variable',\n    0: 'Values'})\n    rent = mean[mean['Variable'].str.contains('rent')]\n    income = mean[mean['Variable'].str.contains('income')]\n\n    rent['Year'] = rent['Variable'].astype(str).str[-4:]\n    income['Year'] = income['Variable'].astype(str).str[-4:]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw={'width_ratios': [1, 1]})\n    sns.barplot(x='Year', y='Values', data=rent, palette='pastel', ax=axes[0])\n    axes[0].set_xlabel('year')\n    axes[0].set_ylabel('Median Rent')\n    axes[0].set_title('Median Rent In ' + park_name + ' Area' , fontsize=10)\n    sns.barplot(x='Year', y='Values', data=income, palette='pastel', ax=axes[1])\n    axes[1].set_xlabel('year')\n    axes[1].set_ylabel('Median Income')\n    axes[1].set_title('Median Income  In ' + park_name + ' Area' , fontsize=10)\n    plt.tight_layout()\n    # Show the plots\n    plt.show()\n\n\n\n\nCode\ncentral = ['013000' , '012000' , '015001' , '015002']\ncompare(nyc_demo_merged_sub_loss, central , 'Central Park')\n\n\n\n\n\n\n\nCode\nbrooklyn = [ '000301' ]\ncompare(nyc_demo_merged_sub_loss, brooklyn , 'Brooklyn Bridge Park')\n\n\n\n\n\n\n\nCode\nwsqaure = [ '006500' ]\ncompare(nyc_demo_merged_sub_loss, wsqaure , 'Washington SquareÂ Park')\n\n\n\n\n\n\n\nCode\nwsqaure = [ '003900' ]\ncompare(nyc_demo_merged_sub_loss, wsqaure , 'Silver Lake Park')\n\n\n\n\n\n\n\nIs there a correlation between median income and the accessibility of parks?\n\n\nCode\nwith open('indicators_data/2388.json', 'r') as file:\n    park_walking_data = json.load(file)\n    \npark_walking_data = pd.DataFrame(park_walking_data)\npark_walking_data = park_walking_data[park_walking_data.GeoType==\"UHF42\"]\n\nshape_file_loc = 'UHF 42/UHF_42_DOHMH.shp'\n#convert it into geopanda dataframe \ndef get_gpd_df(use_shape_file=True):\n    if use_shape_file:\n        gdf = gpd.read_file(shape_file_loc)\n    return gdf\nuhf_gpd =  get_gpd_df()\nuhf_gpd.to_crs(epsg = \"4326\", inplace = True)\n\nmerged_ny_walking = uhf_gpd.merge(\n    park_walking_data,\n    left_on=[\"UHF\"],\n    right_on=[\"GeoID\"])\n\njoined_data = gpd.sjoin(nyc_demo_merged, merged_ny_walking, how='left', op='within')\njoined_data.dropna(inplace=True)\n\nnumerical_correlation, _ = pearsonr(joined_data['Value'], joined_data['median_income2010'])\nprint(f\"Pearson's correlation coefficient: {numerical_correlation}\")\n\nnumerical_correlation, _ = pearsonr(joined_data['Value'], joined_data['median_income2021'])\nprint(f\"Pearson's correlation coefficient: {numerical_correlation}\")\n\nm = mapping(joined_data,\"Value\",parks_gpd=parks_gpd)\n# Save the map as HTML\nhtml_path = \"map1.html\"\nm.save(html_path)\n# Display the HTML file as an iframe\nIFrame(html_path, width=800, height=600)\n\n\nPearson's correlation coefficient: -0.263688800485939\nPearson's correlation coefficient: -0.12378378631103196\n\n\n\n        \n        \n\n\n\n\nIs there a correlation between the presence of families in nearby neighbourhoods and the availability of Children Playgrounds?\n\n\nCode\nshape_file_loc = 'CPA/geo_export_f2e1ecf0-ad20-4676-8b9d-ee96dfeef1bd.shp'\n\n#convert it into geopanda dataframe \ndef get_gpd_df(use_shape_file=True):\n    if use_shape_file:\n        gdf = gpd.read_file(shape_file_loc)\n    return gdf\nCPA_gpd =  get_gpd_df()\n\n\nshape_file_loc = 'Athletic_Facilities/geo_export_35c572bd-a799-446c-890f-c78e4d293ea0.shp'\n\n#convert it into geopanda dataframe \ndef get_gpd_df(use_shape_file=True):\n    if use_shape_file:\n        gdf = gpd.read_file(shape_file_loc)\n    return gdf\nAF_gpd =  get_gpd_df()\n\n#source https://docs.google.com/spreadsheets/d/1iIhwuLBlIus2n1EQ2a329jX4oJciXt9dEaxOFPpHfE8/edit?usp=sharing\nsport_map = {\n    'BKB': 'Basketball',\n    'BSB': 'Baseball',\n    'CRK': 'Cricket',\n    'FFB': 'Flag Football',\n    'FRS': 'Frisbee',\n    'FTB': 'Football',\n    'HDB': 'Handball',\n    'HKY': 'Hockey',\n    'KBL': 'Kickball',\n    'LCS': 'Lacrosse',\n    'MPPA': 'Multi Purpose Play Area',\n    'NTB': 'Netball',\n    'RBY': 'Rugby',\n    'SCR': 'Soccer',\n    'SFB': 'Softball',\n    'TNS': 'Tennis',\n    'TRK': 'Track',\n    'VLB': 'Volleyball',\n    'WFB': 'Wheelchair Football'\n}\n\nAF_gpd['primary_sp'] = AF_gpd['primary_sp'].replace(sport_map)\nAF_count = AF_gpd.groupby('primary_sp').size().reset_index(name='Count')\n\nplt.figure(figsize=(10, 6))\nax = sns.barplot(x='primary_sp', y='Count', data=AF_count, palette='Set2')\nax.set_title('Sports Facilities in NYC Parks' , fontsize=16)\nax.set_xlabel('Sport')\nax.set_ylabel('Freq')\nplt.xticks(rotation=45, ha='right');\n\n\n\n\n\n\n\nCode\nimport folium\nfrom folium.plugins import MarkerCluster\n\nm = folium.Map(location=[40.747993, -74.004890] , tiles=\"Cartodb positron\", zoom_start=10)\nplaygrounds_cluster = MarkerCluster(name='Children Play Area').add_to(m)\nfor idx, row in CPA_gpd.iterrows():\n    folium.Marker(location=[row['geometry'].centroid.y, row['geometry'].centroid.x], popup=row['name']).add_to(playgrounds_cluster)\n\nathletic_cluster = MarkerCluster(name='Athletic Facilities').add_to(m)\nfor idx, row in AF_gpd.iterrows():\n    folium.Marker(location=[row['geometry'].centroid.y, row['geometry'].centroid.x] ,popup=row['primary_sp']).add_to(athletic_cluster)\n\nfolium.LayerControl().add_to(m)\n\nhtml_path = \"FA_CPA_Map.html\"\nm.save(html_path)\n# Display the HTML file as an iframe\nIFrame(html_path, width=800, height=600)\n\n\n\n\n        \n        \n\n\n\n\nCode\nvars_fam = ['NAME', 'B11012_003E', 'B11012_006E', 'B11012_010E','B11012_015E']\n\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nNY_fam_data2021 = acs.query(\n    cols=vars_fam,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": ny_state_code,  \"county\":  \"*\"},\n)\nNY_fam_data2021 = NY_fam_data2021[NY_fam_data2021['county'].isin(ny_city_counties)]\nfor variable in vars_fam:\n    if variable != \"NAME\":\n        NY_fam_data2021[variable] = NY_fam_data2021[variable].astype(float)\n        \nNY_fam_data2021['Total_Families'] = NY_fam_data2021[['B11012_003E', 'B11012_006E', 'B11012_010E','B11012_015E']].sum(axis=1)\n\nnyc_fam_merged = nyc_gpd.merge(\n    NY_fam_data2021,\n    left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\"],\n    right_on=[\"state\", \"county\", \"tract\"],)\n\n\n\n\nCode\njoined_data2 = gpd.sjoin(CPA_gpd, nyc_fam_merged, how=\"inner\", op=\"within\")\njoined_data2.drop_duplicates(subset=['location','county'], inplace=True)\n\nplaygrounds_per_tract = joined_data2.groupby(['tract', 'county']).size().reset_index(name='CPA_Count')\nCPA_fam_count = nyc_fam_merged.merge(playgrounds_per_tract, how='left', left_on=['tract','county'], right_on=['tract','county'])\nCPA_fam_count.dropna(inplace=True)\nCPA_fam_count = CPA_fam_count[CPA_fam_count.Total_Families&gt;0]\n\ncorrelation_coefficient, p_value = pearsonr(CPA_fam_count['CPA_Count'], CPA_fam_count['Total_Families'])\n\ncorrelation_coefficient\n\n\n0.07816702466699046\n\n\n\n\nCode\n%matplotlib inline\nplt.figure(figsize=(7, 3))\nsns.scatterplot(x='Total_Families', y='CPA_Count', data=CPA_fam_count)\nplt.title('Number of Playgrounds vs Total Families')\nplt.xlabel('families count')\nplt.ylabel('Number of children Play Area')\nplt.show()\n\n\n\n\n\n\n\nIs there a corrleation between regular exercise and the avaliability atheltic facilities at parks ?\n\n\nCode\nwith open('indicators_data/2060.json', 'r') as file:\n    physical_activity_data = json.load(file)\n\nphysical_activity_data = pd.DataFrame(physical_activity_data)\nphysical_activity_data = physical_activity_data[physical_activity_data.GeoType==\"UHF42\"]\nphysical_activity_data.GeoID = physical_activity_data.GeoID.astype(float)\n\nphysical_activity_data = physical_activity_data[physical_activity_data.Time=='2019']\nmerged_ny_physical = uhf_gpd.merge(\n    physical_activity_data,\n    left_on=[\"UHF\"],\n    right_on=[\"GeoID\"])\n\njoined_gdf = gpd.sjoin(AF_gpd, merged_ny_physical, how='inner', op='intersects')\n\ntract_aggregated = joined_gdf.groupby(['GeoID','borough' ])['system'].count().reset_index()\n\nmerged_data = pd.merge(tract_aggregated, physical_activity_data, on=['GeoID'])\n\ncorrelation_coefficient, p_value = pearsonr(merged_data['system'], merged_data['Value'])\ncorrelation_coefficient\n\n\n-0.2820924647154564\n\n\n\n\nIs there a corrleation between exercise and accessibility of parks?\n\n\nCode\n#Physical Activity \nwith open('indicators_data/2060.json', 'r') as file:\n    physical_activity_data = json.load(file)\n    \nwith open('indicators_data/2388.json', 'r') as file:\n    park_walking_data = json.load(file)\n    \n#read shape file for UHF 42\nshape_file_loc = 'UHF 42/UHF_42_DOHMH.shp'\n\n#convert it into geopanda dataframe \ndef get_gpd_df(use_shape_file=True):\n    if use_shape_file:\n        gdf = gpd.read_file(shape_file_loc)\n    return gdf\nuhf_gpd =  get_gpd_df()\nuhf_gpd.to_crs(epsg = \"4326\", inplace = True)\n\n\nphysical_activity_data = pd.DataFrame(physical_activity_data)\nphysical_activity_data = physical_activity_data[physical_activity_data.GeoType==\"UHF42\"]\nphysical_activity_data.GeoID = physical_activity_data.GeoID.astype(float)\nmerged_ny_physical = uhf_gpd.merge(\n    physical_activity_data,\n    left_on=[\"UHF\"],\n    right_on=[\"GeoID\"])\n\nplot = merged_ny_physical.hvplot(\n    c=\"Value\",\n    groupby=\"Time\",\n    frame_width=600,\n    frame_height=600,\n    geo=True,\n    dynamic=False,\n    cmap=\"Greens\",\n    hover_cols=[\"GEOID\"]\n)\nplot.opts(title='Recent Exercise in NY by Neighbourhood')\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\n#ref:\n#https://a816-dohbesp.nyc.gov/IndicatorPublic/data-explorer/physical-activity/?id=2060\n\npark_walking_data = pd.DataFrame(park_walking_data)\npark_walking_data = park_walking_data[park_walking_data.GeoType==\"UHF42\"]\n\nphysical_activity_data = physical_activity_data[physical_activity_data.Time=='2017']\n\nmerged_ny_walking = uhf_gpd.merge(\n    park_walking_data,\n    left_on=[\"UHF\"],\n    right_on=[\"GeoID\"])\n\nplot = merged_ny_walking.hvplot(\n    c=\"Value\",\n    groupby=\"Time\",\n    frame_width=600,\n    frame_height=600,\n    geo=True,\n    dynamic=False,\n    cmap=\"Greens\",\n    hover_cols=[\"GEOID\"]\n)\nplot.opts(title='Walking Distance to a Park in NY by Neighbourhood in 2017')\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\nmerged_walking_corr = physical_activity_data.merge(\n    park_walking_data, on=['GeoID', 'Time'])\ncorrelation_coefficient, p_value = pearsonr(merged_walking_corr.Value_x,merged_walking_corr.Value_y)\ncorrelation_coefficient\n\n\n0.28957159180561953\n\n\n\n\nCode\nplt.figure(figsize=(7, 3))\nsns.scatterplot(x='Value_x', y='Value_y', data=merged_walking_corr)\nplt.title('Park Accessibility Vs Exercise rate')\nplt.xlabel('% Of recent exercise per area responses')\nplt.ylabel('% of Accessabile Parks per responses')\nplt.show()"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, letâs load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows Ã 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, letâs load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLetâs explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a âNoteâ callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  }
]